{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cacb3ed-9ff5-436c-a970-1cb8810c9b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from tabnanny import verbose\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9ae82f91-e9b5-4e08-b0a9-fdc45e32e02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QTabAgent:\n",
    "    \"\"\"RL agent for the Roomba\"\"\"\n",
    "\n",
    "    def __init__(self, state_ranges):\n",
    "        dims = [x[1]+1 for x in state_ranges]\n",
    "        dims.append(8)\n",
    "        self.q : np.ndarray = np.ones(dims, dtype=\"float64\")\n",
    "        self.state = 0\n",
    "        self.next_state = 0\n",
    "        self.reward = 0\n",
    "        self.action = 0\n",
    "        self.turn = 0\n",
    "        self.epsilon = 0.2\n",
    "        self.alpha = 0.005\n",
    "        self.gamma = 0.99\n",
    "        self.eta = 0.1\n",
    "        self.number_of_states = self.q.size\n",
    "        self.number_of_actions = 8 # 4 cardinal directions for power on and off\n",
    "        self.verbose = False\n",
    "\n",
    "    def get_number_of_states(self):\n",
    "        return self.number_of_states\n",
    "\n",
    "    def get_number_of_actions(self):\n",
    "        return self.number_of_actions\n",
    "\n",
    "    def e_greedy(self, actions: np.ndarray):\n",
    "        a_star_idx = np.argmax(actions)\n",
    "        rng = np.random.default_rng()\n",
    "        if self.epsilon <= rng.random():\n",
    "            if self.verbose: print('Exploit: {}'.format(a_star_idx))\n",
    "            return a_star_idx\n",
    "        else:\n",
    "            idx = random.randrange(0, len(actions))\n",
    "            if self.verbose: print('Explore: {}'.format(idx))\n",
    "            return idx\n",
    "\n",
    "    def select_action(self, state: tuple[int, int, int, int, int, int, int]) -> int:\n",
    "        self.turn += 1\n",
    "        actions = self.q[state[0], state[1], state[2], state[3], state[4], state[5], state[6], ]\n",
    "        action = self.e_greedy(actions)\n",
    "        return action\n",
    "\n",
    "    def update_q(self, old_state, action, reward, new_state, done):\n",
    "        q = self.q[*old_state, action]\n",
    "        if done:\n",
    "            error = reward - q\n",
    "        else:\n",
    "            q_prime = max(self.q[*new_state, ])\n",
    "            error = reward + self.gamma * q_prime - q\n",
    "        self.q[*old_state, action] = q + self.eta * error\n",
    "\n",
    "    def save_q_table(self, filename=\"q_table\"):\n",
    "        \"\"\"Saves the Q-table to a text file.\n",
    "    \n",
    "        Args:\n",
    "            agent: The agent object containing the Q-table.\n",
    "            filename: The name of the file to save to.\n",
    "        \"\"\"\n",
    "        q_np = self.q  # Access the Q-table (assuming it's a NumPy array)\n",
    "        np.save(filename, q_np)\n",
    "    \n",
    "    def load_q_table(self, filename=\"q_table\"):\n",
    "        \"\"\"Loads the Q-table from a text file.\n",
    "    \n",
    "        Args:\n",
    "            agent: The agent object to load the Q-table into.\n",
    "            filename: The name of the file to load from.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            loaded_q = np.load(filename)\n",
    "            self.q = loaded_q  # Assign the loaded Q-table to the agent\n",
    "            print(f\"Q-table loaded from {filename}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: File {filename} not found.\")\n",
    "        except Exception as e: # Catches other potential errors during loading\n",
    "            print(f\"An error occurred while loading the Q-table: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b1a369ae-dc7b-4606-a04f-23914e6e061c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46e4929a-1a56-45c8-8cf0-02948e39c78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# # # # # # # #\n",
      "# @ . * * * . #\n",
      "# . . * . * * #\n",
      "# . * * . . * #\n",
      "# . . * . . . #\n",
      "# * . . * * . #\n",
      "# . . . . . . #\n",
      "# . . . . . . #\n",
      "# * * * * * * #\n",
      "# * . . . * * #\n",
      "# . . . * . * #\n",
      "# . . . . . . #\n",
      "# . * * . * * #\n",
      "# . . . . . . #\n",
      "# . * * . * . #\n",
      "# # # # # # # #\n",
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generate_roomba_map(size_x: int, size_y: int, dirt_probability:float = 0.2, room_seed: int | None = None,\n",
    "                        dirt_seed: int | None = None) ->tuple[list[list[str]], tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Generates a 16x16 tile map for a Roomba simulation.\n",
    "    \n",
    "    Args:\n",
    "    dirt_probability: The probability of a tile being dirty (float between 0 and 1).\n",
    "    seed: The seed for the random number generator.  Makes generation repeatable.\n",
    "    \n",
    "    Returns:\n",
    "    A 2D list representing the map.\n",
    "    \"\"\"\n",
    "    \n",
    "    random.seed(room_seed)\n",
    "\n",
    "    # Create a 16x16 grid with walls around the perimeter\n",
    "    map = [['#' for _ in range(size_x)] for _ in range(size_y)]\n",
    "\n",
    "    # Choose a random side for the charging station\n",
    "    side = random.randint(0, 3)\n",
    "\n",
    "    rnd_x = random.randint(1, size_x-2) # left-right placement\n",
    "    rnd_y = random.randint(1, size_y-2) # top-down placement\n",
    "    if side == 0:  # Top\n",
    "        map[1][rnd_x] = '@'\n",
    "        start_pos = (1, rnd_x) # first-row, random width placement\n",
    "    elif side == 1:  # Bottom\n",
    "        map[size_y-2][rnd_x] = '@'\n",
    "        start_pos = (size_y-2, rnd_x)\n",
    "    elif side == 2: # Left\n",
    "        map[rnd_y][1] = '@'\n",
    "        start_pos = (rnd_y, 1)\n",
    "    elif side == 3: # Right\n",
    "        map[rnd_y][size_x-2] = '@'\n",
    "        start_pos = (rnd_y, size_x-2)\n",
    "    else:\n",
    "        raise RuntimeError(\"Unexpected side value\")\n",
    "      \n",
    "    random.seed(dirt_seed)\n",
    "      \n",
    "    for row in range(1, size_y-1):\n",
    "        for col in range(1, size_x-1):\n",
    "            if map[row][col] == '@':\n",
    "                continue\n",
    "            # Add a clear tile with a chance of dirt\n",
    "            if random.random() < dirt_probability:\n",
    "                map[row][col] = '*'\n",
    "            else:\n",
    "                map[row][col] = '.'\n",
    "\n",
    "    return map, start_pos\n",
    "\n",
    "# Example usage:\n",
    "room_map, charging_base = generate_roomba_map(8,16, dirt_probability=0.3, room_seed=42, dirt_seed=42)\n",
    "\n",
    "# Print the map\n",
    "for row in room_map:\n",
    "  print(' '.join(row)) \n",
    "print(charging_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab678e4a-8049-4790-b459-6dd7ad2b5b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "import math\n",
    "\n",
    "class RoombaEnv():\n",
    "\n",
    "    NUMBER_OF_ACTIONS = 8 #  N_on, N_off, S_on, S_off, E_on, E_off, W_on, W_off\n",
    "    NUMBER_OF_BATTLVL = 4 # High, Medium, Low, Dead\n",
    "    NUM_CLEANLINESS_LEVELS = 3 # Clean, Dirty, Impassable\n",
    "    BATTERY_SIZE = 4\n",
    "    \n",
    "    LOOK_UP: dict[str, int] = {'#': 0, \"*\": 1, \"@\": 2, \".\": 2}\n",
    "    \n",
    "    def _roomba_reset(self, es_flag: bool, charge: float | None):\n",
    "        \"\"\"\n",
    "        Reset roomba\n",
    "        :param es_flag: exploring starts \n",
    "        :param charge: initial charge percentage (float between 0 and 1)\n",
    "        \"\"\"\n",
    "        if es_flag:\n",
    "            while True:\n",
    "                pos = (random.randint(0, len(self.map[0])), random.randint(0, len(self.map)))\n",
    "                if self.map[pos[0]][pos[1]] != '#':\n",
    "                    break\n",
    "        else:\n",
    "            pos = self.charge_loc\n",
    "        \n",
    "        self.pos: tuple[int, int] = pos\n",
    "        \n",
    "        self.battery_lvl: float = math.ceil(len(self.map) * len(self.map[0]) * self.BATTERY_SIZE * charge)\n",
    "        self.battery_cap: float = len(self.map) * len(self.map[0]) * self.BATTERY_SIZE\n",
    "        self.clean_success: bool = False        \n",
    "        self.prev_action = None\n",
    "        self.curr_action = None\n",
    "                \n",
    "\n",
    "    def __init__(self, map: list[list[str]], charging_location: (int, int), \n",
    "                 es_flag: bool = False, charge: float | None = 1, verbose: bool = False):\n",
    "        \"\"\"\n",
    "        Set class properties with environment constants\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.verbose = verbose\n",
    "        self.starting_map = copy.deepcopy(map)\n",
    "        self.map = map\n",
    "        self.charge_loc = charging_location\n",
    "        self.orig_dirty_count = self.get_dirty_count()\n",
    "        self.wall_hit = False\n",
    "        # assumes rectangular map\n",
    "        self.total_states = (len(map) * len(map[0]) * \n",
    "                             self.NUMBER_OF_BATTLVL * pow(self.NUM_CLEANLINESS_LEVELS, 4))        \n",
    "        self._roomba_reset(es_flag, charge)\n",
    "        \n",
    "\n",
    "    def get_number_of_states(self) -> int:\n",
    "        # Environment constant pass thru\n",
    "        return self.total_states\n",
    "    \n",
    "    def get_state_dims(self) -> tuple[tuple[int, int], tuple[int, int], \n",
    "                                      tuple[int, int], tuple[int, int],\n",
    "                                      tuple[int, int], tuple[int, int], tuple[int, int]]:\n",
    "        \"\"\"\n",
    "        Get the dimensions of each parameter of the state.  This helps initialize a model for many algorithms\n",
    "        :return: Tuple of tuples representing the range of each parameter of the state.\n",
    "        \"\"\"\n",
    "        return ((0, len(self.map[0])-1), (0, len(self.map)-1), (0, self.NUMBER_OF_BATTLVL-1), \n",
    "                (0, 2), (0, 2), (0, 2), (0, 2))\n",
    "\n",
    "    def get_number_of_actions(self) -> int:\n",
    "        # Environment constant pass thru\n",
    "        return self.NUMBER_OF_ACTIONS\n",
    "\n",
    "    def reset(self, es_flag: bool = False, charge: float | None = 1) \\\n",
    "            -> tuple[int, int, int, int, int, int, int]:\n",
    "        \"\"\"\n",
    "        Reset the state of the game to a determined start_state if es_flag is False\n",
    "        Otherwise if es_flag is True then reset game to a random start state\n",
    "        Return the resulting state for agent to act on.\n",
    "        \"\"\"\n",
    "        if charge is None:\n",
    "            charge = random.random()\n",
    "        self.map = copy.deepcopy(self.starting_map)\n",
    "        self.orig_dirty_count = self.get_dirty_count()\n",
    "        self._roomba_reset(es_flag, charge)\n",
    "        \n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self) -> tuple[int, int, int, int, int, int, int]:\n",
    "        \"\"\"\n",
    "        Return current environment state.\n",
    "        Return a tuple of the current state of the roomba. (x, y, charge, 4 x [peeks])\n",
    "        \"\"\"\n",
    "        return (self.pos[0], self.pos[1], \n",
    "                int(math.ceil(self.battery_lvl/(self.battery_cap/3))), \n",
    "                self.LOOK_UP[self.map[self.pos[0]][self.pos[1]+1]], # Look East\n",
    "                self.LOOK_UP[self.map[self.pos[0]+1][self.pos[1]]], # Look South\n",
    "                self.LOOK_UP[self.map[self.pos[0]][self.pos[1]-1]], # Look West\n",
    "                self.LOOK_UP[self.map[self.pos[0]-1][self.pos[1]]]) # Look North\n",
    "\n",
    "    def get_optimal_hints(self, agent):\n",
    "        max_dist = 0\n",
    "        min_dist = 1e9\n",
    "        for row in range(len(self.map)):\n",
    "            for col in range(len(self.map[0])):\n",
    "                if self.map[row][col] == '#':\n",
    "                    agent.q[row,col] = -100\n",
    "\n",
    "    def step(self, action: int) -> (tuple[int, int, int, int, int, int, int], float, bool):\n",
    "        \"\"\"\n",
    "        Given an action, determine the resulting next_state. \n",
    "        Based on next_state determine the resulting reward for getting there.\n",
    "        Update current environment state and find if it is an end state.\n",
    "        \"\"\"\n",
    "        next_state = self._perform_action(action)\n",
    "        done = self._get_terminal_flag()\n",
    "        reward = self._get_reward(action, done)\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def _perform_action(self, action: int) ->  tuple[int, int, int, int, int, int, int]:\n",
    "        \"\"\"\n",
    "        action: 'N_off'=0, 'S_off'=1, 'E_off'=2, 'W_off'=3, 'N_on'=4, 'S_on'=5, 'E_on'=6, 'W_on'=7\n",
    "        \"\"\"\n",
    "        self.prev_action = self.curr_action\n",
    "        self.curr_action = action\n",
    "        \n",
    "        ## Find battery level\n",
    "        power_on = action > 3\n",
    "        self.battery_lvl -= 0.09 * 10 # default power reduction\n",
    "        if power_on:\n",
    "            self.battery_lvl -= 0.1 * 10 # power is on, reduce power more\n",
    "        self.battery_lvl = max(self.battery_lvl, 0)\n",
    "        if self.verbose:\n",
    "            print(\"Battery level: \", self.battery_lvl)\n",
    "\n",
    "        ## Check map movement\n",
    "        new_loc = [self.pos[0], self.pos[1]]\n",
    "        old_loc = new_loc.copy() # For verbose tracking purposes\n",
    "        action_mod = action % 4\n",
    "        if action_mod == 0:\n",
    "            new_loc[0] -= 1\n",
    "            if self.verbose: print('North')\n",
    "        elif action_mod == 1:\n",
    "            new_loc[0] += 1\n",
    "            if self.verbose: print('South')\n",
    "        elif action_mod ==2:\n",
    "            new_loc[1] += 1\n",
    "            if self.verbose: print('East')\n",
    "        else:\n",
    "            new_loc[1] -= 1\n",
    "            if self.verbose: print('West')\n",
    "        new_loc_tile = self.map[new_loc[0]][new_loc[1]]\n",
    "        \n",
    "        self.clean_success = False\n",
    "        self.wall_hit = False\n",
    "        if new_loc_tile != '#':\n",
    "            self.pos = (new_loc[0], new_loc[1]) # not a wall so update\n",
    "            if self.verbose: print(self.pos)\n",
    "            if new_loc_tile == '*' and power_on:\n",
    "                self.map[self.pos[0]][self.pos[1]] = '.'\n",
    "                self.clean_success = True            \n",
    "        else:\n",
    "            self.wall_hit = True\n",
    "            if self.verbose:\n",
    "                print('Hit a wall, revert')\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"Current spot: {} ({},{})\".format(self.map[old_loc[0]][old_loc[1]], \n",
    "                                                    old_loc[0], old_loc[1]))\n",
    "            print(\"Next spot: {} ({},{})\".format(new_loc_tile, new_loc[0], new_loc[1]))\n",
    "            xmap = copy.deepcopy(self.map)\n",
    "            xmap[self.pos[0]][self.pos[1]] = 'O'\n",
    "            for row in xmap:\n",
    "                print(' '.join(row))\n",
    "        \n",
    "        return self.get_state()\n",
    "\n",
    "    def _get_reward(self, action: int, term: bool) -> float:\n",
    "        \"\"\"\n",
    "        Determines reward for agent\n",
    "        \"\"\"\n",
    "        batt_state = int(math.ceil(self.battery_lvl/(self.battery_cap/3)))\n",
    "        reward = -1.0\n",
    "        if self.wall_hit:\n",
    "             reward -= 10\n",
    "        \n",
    "        if action > 3:\n",
    "            if self.clean_success:\n",
    "                dist = ((self.charge_loc[0] - self.pos[0])**2 + (self.charge_loc[1] - self.pos[1])**2)**0.5\n",
    "                if batt_state == 3:\n",
    "                    reward += (6 * dist)\n",
    "                elif batt_state == 2:\n",
    "                    reward += (1 * dist)\n",
    "                elif batt_state == 1:\n",
    "                    reward += (1 / dist)\n",
    "            else:\n",
    "                reward -= 10\n",
    "                \n",
    "        if term:\n",
    "            dirty_count = sum([x.count('*') for x in self.map])\n",
    "            if self.orig_dirty_count > 0:\n",
    "                dirty_count /= self.orig_dirty_count\n",
    "            if self.battery_lvl == 0:\n",
    "                reward -= (dirty_count * 100)\n",
    "            elif batt_state == 3:\n",
    "                reward -= dirty_count * 50\n",
    "            else:\n",
    "                reward -= dirty_count * 10\n",
    "\n",
    "        if self.prev_action is not None and self.curr_action % 4 == self.prev_action % 4:\n",
    "            reward += 0\n",
    "        elif not self.clean_success:\n",
    "            reward -= 10\n",
    "        return reward\n",
    "\n",
    "    def _get_terminal_flag(self) -> bool:\n",
    "        \"\"\"\n",
    "        Return if current state is in a list of set terminal states\n",
    "        \"\"\"\n",
    "        return self.map[self.pos[0]][self.pos[1]] == '@' or self.battery_lvl == 0\n",
    "    \n",
    "    def get_clean_level(self) -> float:\n",
    "        return 1- sum([x.count('*') for x in self.map]) / (len(self.map) * len(self.map[0]))\n",
    "    \n",
    "    def get_dirty_count(self) -> float:\n",
    "        return sum([x.count('*') for x in self.map])\n",
    "\n",
    "    def get_dirty_pct(self) -> float:\n",
    "        if not self.orig_dirty_count or self.orig_dirty_count == 0:\n",
    "            return 1.\n",
    "        return self.get_dirty_count() / self.orig_dirty_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d6d463-60fd-4166-8bbf-2ea37e7b70fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to train on different maps, full and sparse, and across the 3 different power levels.\n",
    "from tqdm import tqdm\n",
    "\n",
    "SEED = 42\n",
    "SIZE = 16\n",
    "#agent = None\n",
    "episodes = 200000\n",
    "\n",
    "def main():\n",
    "    global agent # for debugging, keeping agent to call in later cells\n",
    "    \n",
    "    #show map\n",
    "    disp_map, charging_base = generate_roomba_map(SIZE, SIZE, dirt_probability=0.3, room_seed=SEED)\n",
    "    for row in disp_map:\n",
    "        print(' '.join(row))\n",
    "    \n",
    "    environment = RoombaEnv(disp_map, charging_base)\n",
    "    if agent is None:\n",
    "        agent = QTabAgent(environment.get_state_dims())\n",
    "        environment.get_optimal_hints(agent)\n",
    "    for i in tqdm(range(episodes)):\n",
    "        disp_map, charging_base = generate_roomba_map(SIZE, SIZE, dirt_probability=max(random.random(), 0.37), room_seed=SEED)\n",
    "        environment.starting_map = disp_map\n",
    "        environment.charging_base = charging_base\n",
    "        # reset the game and observe the current state\n",
    "        agent.epsilon = max(0.75 - (i/(episodes/2)*0.75), 0.01) # testing out episodic decay of epsilon\n",
    "        current_state = environment.reset()\n",
    "        game_end = False\n",
    "        total_reward = 0\n",
    "        # Do until the game ends:\n",
    "        while not game_end:\n",
    "            action = agent.select_action(current_state)\n",
    "            new_state, reward, game_end = environment.step(action)\n",
    "            agent.update_q(current_state, action, reward, new_state, game_end)\n",
    "            current_state = new_state\n",
    "            total_reward += reward\n",
    "        if i % 5000 == 0:\n",
    "            #pass\n",
    "            print(\"Game Over, clean level: {}, dirty count: {}, dirty start: {}, dirty pct: {}, reward: {}\".\n",
    "                format(environment.get_clean_level(),\n",
    "                environment.get_dirty_count(),\n",
    "                environment.orig_dirty_count,\n",
    "                environment.get_dirty_pct(),\n",
    "                total_reward))\n",
    "    #with open('Project1.txt', 'wt') as f:\n",
    "    #    print(agent.q, file=f)\n",
    "        \n",
    "    print(\"Test the policy: \")\n",
    "    agent.verbose = True\n",
    "    agent.epsilon = 0\n",
    "    current_state = environment.reset()\n",
    "    environment.verbose = True\n",
    "    game_end = False\n",
    "    # Do until the game ends:\n",
    "    count = 0\n",
    "    while not game_end:\n",
    "        action = agent.select_action(current_state)\n",
    "        new_state, reward, game_end = environment.step(action)\n",
    "        current_state = new_state\n",
    "        count += 1\n",
    "        # Allow verbose to see maps, but not every map, too much\n",
    "        if count % 100 == 0:\n",
    "            agent.verbose = True\n",
    "        else:\n",
    "            agent.verbose = False\n",
    "    print(\"\\nProgram completed successfully.\")\n",
    "    for row in environment.map:\n",
    "        print(' '.join(row))\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b9d6e7a4-2cbd-4fd4-9c51-01f8fda14e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = QTabAgent(environment.get_state_dims())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d4f576eaa4b1da02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table loaded from Roomba97_16x16.npy\n"
     ]
    }
   ],
   "source": [
    "SIZE = 16\n",
    "agent.load_q_table('Roomba97_16x16.npy')\n",
    "#agent.save_q_table('Roomba9737_16x16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ae4aaddd-c070-4546-8dd3-143356028e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table loaded from Roomba1_8x8.npy\n"
     ]
    }
   ],
   "source": [
    "SIZE = 8\n",
    "agent.load_q_table('Roomba1_8x8.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0842f9da-3b20-4bbd-819f-537fcd0a386f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# independently test other maps with trained agent\n",
    "print(\"Test the policy: \")\n",
    "disp_map, charging_base = generate_roomba_map(SIZE, SIZE, dirt_probability=0.9, room_seed=SEED)\n",
    "environment = RoombaEnv(disp_map, charging_base)\n",
    "agent.verbose = True\n",
    "agent.epsilon = 0. # Full exploit?\n",
    "current_state = environment.reset()\n",
    "environment.verbose = True\n",
    "game_end = False\n",
    "count = 0\n",
    "# Do until the game ends:\n",
    "while not game_end:\n",
    "    action = agent.select_action(current_state)\n",
    "    new_state, reward, game_end = environment.step(action)\n",
    "    current_state = new_state\n",
    "    count += 1\n",
    "    # Allow verbose to see maps, but not every map, too much\n",
    "    if count % 100 == 0:\n",
    "        agent.verbose = True\n",
    "    else:\n",
    "        agent.verbose = False\n",
    "print(\"\\nProgram completed successfully.\")\n",
    "for row in environment.map:\n",
    "    print(' '.join(row))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
